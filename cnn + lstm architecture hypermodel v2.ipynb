{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN + LSTM Hypermodel V2\n",
    "The second revision of this hypermodel takes out the sequnce length and vocab size from the tuning so that only the pre-trained models performance is evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\james\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# general imports\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from pickle import load\n",
    "from math import ceil\n",
    "from numpy.random import choice\n",
    "\n",
    "# model imports\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, LSTM, Dropout, Embedding, Add\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.regularizers import L2\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# pre-trained cnn imports \n",
    "from keras.applications import VGG16, VGG19, ResNet50, ResNet152, ResNet50V2, ResNet152V2, InceptionV3, InceptionResNetV2, EfficientNetB0, EfficientNetB1, EfficientNetB2, NASNetMobile, ConvNeXtTiny, ConvNeXtBase, DenseNet121, DenseNet201   # type: ignore\n",
    "from keras.applications import vgg16, vgg19, resnet, resnet_v2, inception_v3, inception_resnet_v2, efficientnet, nasnet, convnext, densenet\n",
    "\n",
    "# tuning imports\n",
    "from keras_tuner import HyperModel, RandomSearch\n",
    "\n",
    "# custom function imports\n",
    "from functions.dataset_loading import load_flicker8k_split\n",
    "from functions.text_processing import create_vocab_mappings\n",
    "from functions.image_processing import load_image, display_image\n",
    "from functions.training import data_generator\n",
    "from functions.model_evaluation import generate_and_evaluate_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 20\n",
    "HIDDEN_DIM = 1024\n",
    "EMBED_DIM = 300\n",
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Fliker 8k pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Flicker8K processed data from folder \n",
    "DIR = 'preprocessed_data/flicker8k/'\n",
    "\n",
    "with open(DIR + 'caption_map.pkl', 'rb') as file:\n",
    "    caption_map = load(file)\n",
    "    \n",
    "with open(DIR + 'feature_map.pkl', 'rb') as file:\n",
    "    feature_map = load(file)\n",
    "\n",
    "with open(DIR + 'embedding_matrix_glove_300.pkl', 'rb') as file:\n",
    "    embedding_matrix = load(file)\n",
    "    \n",
    "with open(DIR + 'vocab.pkl', 'rb') as file:\n",
    "    vocab = load(file)\n",
    "    \n",
    "train_images, val_images, test_images = load_flicker8k_split()\n",
    "\n",
    "STEPS = ceil(len(train_images) * 5 / BATCH_SIZE)\n",
    "VAL_STEPS = ceil(len(val_images) * 5 / BATCH_SIZE)\n",
    "\n",
    "word_to_idx, idx_to_word = create_vocab_mappings(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the image data for train and validation images\n",
    "train_images, val_images, test_images = load_flicker8k_split()\n",
    "\n",
    "train_images = train_images[:500]\n",
    "val_images = val_images \n",
    "\n",
    "train_image_data, val_image_data, test_image_data = {}, {}, {}\n",
    "\n",
    "for img in train_images:\n",
    "    image_path = os.path.join('datasets/flicker8k/Flicker8k_images', img)\n",
    "    image_data = tf.io.read_file(image_path)\n",
    "    image_data = tf.image.decode_jpeg(image_data, channels=3)\n",
    "    image_data = tf.image.resize(image_data, [224, 224])\n",
    "    train_image_data[img] = image_data\n",
    "    \n",
    "for img in val_images:\n",
    "    image_path = os.path.join('datasets/flicker8k/Flicker8k_images', img)\n",
    "    image_data = tf.io.read_file(image_path)\n",
    "    image_data = tf.image.decode_jpeg(image_data, channels=3)\n",
    "    image_data = tf.image.resize(image_data, [224, 224])\n",
    "    val_image_data[img] = image_data\n",
    "    \n",
    "for img in test_images:\n",
    "    image_path = os.path.join('datasets/flicker8k/Flicker8k_images', img)\n",
    "    image_data = tf.io.read_file(image_path)\n",
    "    image_data = tf.image.decode_jpeg(image_data, channels=3)\n",
    "    image_data = tf.image.resize(image_data, [224, 224])\n",
    "    test_image_data[img] = image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Creating training and validation datasets\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mdata_generator\u001b[49m(train_images, caption_map, train_image_data, BATCH_SIZE)\n\u001b[0;32m      3\u001b[0m val_data \u001b[38;5;241m=\u001b[39m data_generator(val_images, caption_map, val_image_data, BATCH_SIZE)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_generator' is not defined"
     ]
    }
   ],
   "source": [
    "# Creating training and validation datasets\n",
    "train_data = data_generator(train_images, caption_map, train_image_data, BATCH_SIZE)\n",
    "val_data = data_generator(val_images, caption_map, val_image_data, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Captioning Hypermodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHyperModel(HyperModel):\n",
    "    def __init__(self, seq_len, vocab_size, embed_dim, embedding_matrix, image_shape= (224, 224, 3), hidden_dim= 1024, dropout= 0.3, seed = 17):\n",
    "        super().__init__()\n",
    "        self.image_shape = image_shape\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = dropout \n",
    "        self.seed = seed\n",
    "        \n",
    "        self.cnn_configs = {\n",
    "            'vgg16': (VGG16, vgg16.preprocess_input),\n",
    "            'vgg19': (VGG19, vgg19.preprocess_input),\n",
    "            'resnet50': (ResNet50, resnet.preprocess_input),\n",
    "            'resnet152': (ResNet152, resnet.preprocess_input),\n",
    "            'resnet50v2': (ResNet50V2, resnet_v2.preprocess_input),\n",
    "            'resnet152v2': (ResNet152V2, resnet_v2.preprocess_input),\n",
    "            'inception': (InceptionV3, inception_v3.preprocess_input),\n",
    "            'inceptionresnet': (InceptionResNetV2, inception_resnet_v2.preprocess_input),\n",
    "            'efficientb0': (EfficientNetB0, efficientnet.preprocess_input),\n",
    "            'efficientb1': (EfficientNetB1, efficientnet.preprocess_input),\n",
    "            'efficientb2': (EfficientNetB2, efficientnet.preprocess_input),\n",
    "            'nasnetmobile': (NASNetMobile, nasnet.preprocess_input),\n",
    "            'densenet121': (DenseNet121, densenet.preprocess_input),\n",
    "            'densenet201': (DenseNet201, densenet.preprocess_input), \n",
    "            'convnexttiny': (ConvNeXtTiny, convnext.preprocess_input),\n",
    "            'convnextbase': (ConvNeXtBase, convnext.preprocess_input)\n",
    "        }   \n",
    "        \n",
    "        # embedding layer\n",
    "        self.embedding_layer = Embedding(\n",
    "            self.vocab_size,\n",
    "            embed_dim,\n",
    "            input_length=self.seq_len,\n",
    "            trainable=False,\n",
    "            weights=[embedding_matrix],\n",
    "            name='sequence_embedding'\n",
    "        )     \n",
    "                \n",
    "    def build(self, hp):\n",
    "        # CNN choice\n",
    "        cnn_choice = hp.Choice('cnn', list(self.cnn_configs.keys()))\n",
    "        CNN, preprocess_func = self.cnn_configs[cnn_choice]\n",
    "        \n",
    "        feature_extractor = CNN(\n",
    "            weights='imagenet',\n",
    "            include_top=False,\n",
    "            input_shape=self.image_shape,\n",
    "            pooling='avg'\n",
    "        )\n",
    "        feature_extractor.trainable = False\n",
    "                \n",
    "        # Image path\n",
    "        image_input = Input(shape=self.image_shape, name='image_input')\n",
    "        processed_image = preprocess_func(image_input)\n",
    "        image_features = feature_extractor(processed_image)        \n",
    "        featuers_dense = Dense(self.hidden_dim, activation = 'relu', name='image_dense', kernel_regularizer=L2(1e-4))(image_features)\n",
    "\n",
    "        # Caption path\n",
    "        caption_input = Input(shape=(self.seq_len,), name='caption_input')\n",
    "        caption_embedding = self.embedding_layer(caption_input)\n",
    "        caption_dropout = Dropout(self.dropout, name='embedding_dropout')(caption_embedding)\n",
    "        lstm = LSTM(self.hidden_dim, return_sequences=True, name='lstm_layer', kernel_regularizer=L2(1e-4))(caption_dropout)        \n",
    "        \n",
    "        # Combine image and sequence\n",
    "        merging_layer = Add()([featuers_dense, lstm])\n",
    "        merge_dropout = Dropout(self.dropout)(merging_layer)\n",
    "\n",
    "        # Prediction layers        \n",
    "        dense_layer = Dense(self.hidden_dim, activation='relu', kernel_regularizer=L2(1e-4))(merge_dropout)\n",
    "        prediction_dropout = Dropout(self.dropout)(dense_layer)\n",
    "        output = Dense(self.vocab_size, activation='softmax', kernel_regularizer=L2(1e-4))(prediction_dropout)\n",
    "        \n",
    "        decoder = Model(inputs = [image_input, caption_input] , outputs = output)       \n",
    "        decoder.compile(optimizer= Adam(1e-3), loss=SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "        \n",
    "        return decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 16 Complete [00h 02m 22s]\n",
      "val_loss: 2.6741182804107666\n",
      "\n",
      "Best val_loss So Far: 2.597045421600342\n",
      "Total elapsed time: 00h 54m 44s\n",
      "Results summary\n",
      "Results in keras_tuning\\cnn_lstm_architecture_search_v2.2\n",
      "Showing 5 best trials\n",
      "Objective(name=\"val_loss\", direction=\"min\")\n",
      "\n",
      "Trial 006 summary\n",
      "Hyperparameters:\n",
      "cnn: densenet121\n",
      "Score: 2.597045421600342\n",
      "\n",
      "Trial 010 summary\n",
      "Hyperparameters:\n",
      "cnn: inception\n",
      "Score: 2.5986948013305664\n",
      "\n",
      "Trial 003 summary\n",
      "Hyperparameters:\n",
      "cnn: densenet201\n",
      "Score: 2.603595018386841\n",
      "\n",
      "Trial 008 summary\n",
      "Hyperparameters:\n",
      "cnn: resnet152\n",
      "Score: 2.6454343795776367\n",
      "\n",
      "Trial 011 summary\n",
      "Hyperparameters:\n",
      "cnn: resnet50\n",
      "Score: 2.649353504180908\n"
     ]
    }
   ],
   "source": [
    "# Initialize hypermodel\n",
    "hypermodel = MyHyperModel(SEQ_LENGTH, VOCAB_SIZE, EMBED_DIM, embedding_matrix)\n",
    "\n",
    "# Initialize tuner\n",
    "tuner = RandomSearch(\n",
    "    hypermodel,\n",
    "    objective=\"val_loss\", \n",
    "    max_trials=200,  \n",
    "    directory=\"keras_tuning\",\n",
    "    project_name=\"cnn_lstm_architecture_search_v2.2\",  \n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Initializing tensorborad callback \n",
    "tensorboard = TensorBoard(\n",
    "    log_dir='./tensorboard_logs/architecture_search_v2.2',\n",
    "    histogram_freq=1,\n",
    "    write_graph=True,\n",
    "    update_freq='epoch'\n",
    ")\n",
    "\n",
    "# Searching for optimal solution \n",
    "tuner.search(\n",
    "    train_data,\n",
    "    epochs = 3,\n",
    "    steps_per_epoch=STEPS,\n",
    "    validation_data=val_data,\n",
    "    validation_steps=VAL_STEPS,\n",
    "    callbacks= [tensorboard],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in keras_tuning\\cnn_lstm_architecture_search_v2.2\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_loss\", direction=\"min\")\n",
      "\n",
      "Trial 006 summary\n",
      "Hyperparameters:\n",
      "cnn: densenet121\n",
      "Score: 2.597045421600342\n",
      "\n",
      "Trial 010 summary\n",
      "Hyperparameters:\n",
      "cnn: inception\n",
      "Score: 2.5986948013305664\n",
      "\n",
      "Trial 003 summary\n",
      "Hyperparameters:\n",
      "cnn: densenet201\n",
      "Score: 2.603595018386841\n",
      "\n",
      "Trial 008 summary\n",
      "Hyperparameters:\n",
      "cnn: resnet152\n",
      "Score: 2.6454343795776367\n",
      "\n",
      "Trial 011 summary\n",
      "Hyperparameters:\n",
      "cnn: resnet50\n",
      "Score: 2.649353504180908\n",
      "\n",
      "Trial 007 summary\n",
      "Hyperparameters:\n",
      "cnn: nasnetmobile\n",
      "Score: 2.6657721996307373\n",
      "\n",
      "Trial 015 summary\n",
      "Hyperparameters:\n",
      "cnn: efficientb1\n",
      "Score: 2.6741182804107666\n",
      "\n",
      "Trial 014 summary\n",
      "Hyperparameters:\n",
      "cnn: inceptionresnet\n",
      "Score: 2.678245782852173\n",
      "\n",
      "Trial 005 summary\n",
      "Hyperparameters:\n",
      "cnn: convnextbase\n",
      "Score: 2.7098047733306885\n",
      "\n",
      "Trial 004 summary\n",
      "Hyperparameters:\n",
      "cnn: efficientb0\n",
      "Score: 2.711036443710327\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training outputs \n",
    "1. ([0.6405022734089714, 0.4463027082229372, 0.29934462206575563, 0.20871405332039702], 0.4145237453375769) v1\n",
    "2. ([0.6418743284682035, 0.442693660623719, 0.29415405024196956, 0.20058639500348188], 0.4080181609786798) v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.model_evaluation import batch_generate_captions, evaluate_captions\n",
    "\n",
    "predicted_captions, true_captions = batch_generate_captions(val_images, val_feature_map, val_caption_map, idx_to_word, best_decoder, SEQ_LEN, 1024)\n",
    "evaluate_captions(predicted_captions, true_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ([0.6211531810024086, 0.410840029257982, 0.26646794915971794, 0.1811533502178472], 0.38122496568020725) batch = 64\n",
    "- ([0.5993318816719131,\n",
    "  0.3865770367462274,\n",
    "  0.24482775805037613,\n",
    "  0.16440581561248588],\n",
    " 0.3609836026638695) batch = 32\n",
    " - ([0.6344307633588684,\n",
    "  0.4343686956410259,\n",
    "  0.28711750048084483,\n",
    "  0.19787981935572438],\n",
    " 0.3971223813690533) batch = 256\n",
    "- ([0.6298820729985007,\n",
    "  0.4312138237301613,\n",
    "  0.2866155554360642,\n",
    "  0.19793620298236123],\n",
    " 0.39902906753744116) previous with more training \n",
    " - ([0.6428814404792379,\n",
    "  0.4456909514391105,\n",
    "  0.29858524736881525,\n",
    "  0.2072303796603672],\n",
    " 0.4091058296690576) batch = 256, 20 epochs, model saved\n",
    " - ([0.6428814404792379,\n",
    "  0.4456909514391105,\n",
    "  0.29858524736881525,\n",
    "  0.2072303796603672],\n",
    " 0.4091058296690576) previous model with more training \n",
    " - ([0.6390607878281732,\n",
    "  0.4406290889734613,\n",
    "  0.2962323225086467,\n",
    "  0.2063178068601112],\n",
    " 0.40974129391739644) changed hid dim to 512\n",
    " - ([0.6568510950992849,\n",
    "  0.46559990385495154,\n",
    "  0.3192417928203884,\n",
    "  0.22491635004582852],\n",
    " 0.42799321255907147) added dense layer after concat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder.save('models/lstm_decoder_for_cnn_encoder.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
