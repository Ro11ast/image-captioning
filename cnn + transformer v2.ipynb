{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\james\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from pickle import load\n",
    "from math import ceil\n",
    "from numpy.random import choice\n",
    "from keras import Model\n",
    "from keras.applications import ResNet152V2\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Add, Dropout\n",
    "from keras import layers\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.applications import InceptionResNetV2\n",
    "\n",
    "from functions.text_processing import create_vocab_mappings\n",
    "from functions.image_processing import display_image\n",
    "from functions.training import data_generator\n",
    "from functions.model_evaluation import generate_and_evaluate_caption\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from functions.model_evaluation import generate_captions, evaluate_captions\n",
    "\n",
    "\n",
    "# from keras.mixed_precision import set_global_policy\n",
    "# set_global_policy(\"mixed_float16\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 15\n",
    "EMBED_DIM = 300\n",
    "FF_DIM = 512\n",
    "HIDDEN_DIM = 512\n",
    "EPOCHS = 100\n",
    "DROPOUT = 0.4\n",
    "BATCH_SIZE = 64\n",
    "EVAL_BATCH_SIZE = 1024\n",
    "DIR = 'preprocessed_data/coco/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('preprocessed_data/vocab.pkl', 'rb') as file:\n",
    "    vocab = load(file)\n",
    "    \n",
    "VOCAB_SIZE = len(vocab)\n",
    "    \n",
    "with open('preprocessed_data/embedding_matrix.pkl', 'rb') as file:\n",
    "    embedding_matrix = load(file)\n",
    "    \n",
    "EMBED_DIM = embedding_matrix.shape[1]\n",
    "\n",
    "word_to_idx, idx_to_word = create_vocab_mappings(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m     train_caption_map \u001b[38;5;241m=\u001b[39m load(file)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(DIR \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_avg_feature_map.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m----> 5\u001b[0m     train_feature_map \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m train_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(train_caption_map\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m      8\u001b[0m STEPS \u001b[38;5;241m=\u001b[39m ceil(\u001b[38;5;28mlen\u001b[39m(train_images) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m/\u001b[39m BATCH_SIZE)\n",
      "File \u001b[1;32mc:\\Projects\\coursework\\.venv\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:178\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner_wrapper\u001b[39m(func):\n\u001b[1;32m--> 178\u001b[0m   \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    179\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m enabled:\n\u001b[0;32m    181\u001b[0m       \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open(DIR + 'train_caption_map.pkl', 'rb') as file:\n",
    "    train_caption_map = load(file)\n",
    "    \n",
    "with open(DIR + 'train_avg_feature_map.pkl', 'rb') as file:\n",
    "    train_feature_map = load(file)\n",
    "    \n",
    "train_images = list(train_caption_map.keys())\n",
    "STEPS = ceil(len(train_images) * 5 / BATCH_SIZE)\n",
    "    \n",
    "train_data = data_generator (train_images, train_caption_map, train_feature_map, BATCH_SIZE)\n",
    "\n",
    "with open(DIR + 'val_caption_map.pkl', 'rb') as file:\n",
    "    val_caption_map = load(file)\n",
    "    \n",
    "with open(DIR + 'val_avg_feature_map.pkl', 'rb') as file:\n",
    "    val_feature_map = load(file)\n",
    "    \n",
    "val_images = list(val_caption_map.keys())\n",
    "VAL_STEPS = ceil(len(val_images) * 5 / BATCH_SIZE)\n",
    "val_data = data_generator(val_images, val_caption_map, val_feature_map, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    with open(DIR + 'train_caption_map.pkl', 'rb') as file:\n",
    "        train_caption_map = load(file)\n",
    "        \n",
    "    with open(DIR + 'train_avg_feature_map.pkl', 'rb') as file:\n",
    "        train_feature_map = load(file)\n",
    "    \n",
    "train_images = list(train_caption_map.keys())\n",
    "STEPS = ceil(len(train_images) * 5 / BATCH_SIZE)\n",
    "    \n",
    "train_data = data_generator (train_images, train_caption_map, train_feature_map, BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    with open(DIR + 'val_caption_map.pkl', 'rb') as file:\n",
    "        val_caption_map = load(file)\n",
    "        \n",
    "    with open(DIR + 'val_feature_map.pkl', 'rb') as file:\n",
    "        val_feature_map = load(file)\n",
    "    \n",
    "val_images = list(val_caption_map.keys())\n",
    "VAL_STEPS = ceil(len(val_images) * 5 / BATCH_SIZE)\n",
    "val_data = data_generator(val_images, val_caption_map, val_feature_map, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The layers: \n",
    "- TransformerEncoderBlock\n",
    "- PositionalEmbedding\n",
    "- TransformerDecoderBlock \n",
    "\n",
    "Are from the Keras image captioning example\n",
    "- Author: A_K_Nain\n",
    "- Source: https://keras.io/examples/vision/image_captioning/#building-the-model\n",
    "'''\n",
    "\n",
    "class TransformerEncoderBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.0\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.dense_1 = layers.Dense(embed_dim, activation=\"relu\")\n",
    "\n",
    "    def call(self, inputs, training, mask=None):\n",
    "        inputs = self.layernorm_1(inputs)\n",
    "        inputs = self.dense_1(inputs)\n",
    "        \n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=None,\n",
    "            training=training,\n",
    "        )\n",
    "        out_1 = self.layernorm_2(inputs + attention_output_1)\n",
    "        return out_1\n",
    "\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, embedding_matrix, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = Embedding(\n",
    "            input_dim=vocab_size, \n",
    "            output_dim=embed_dim,\n",
    "            input_length=sequence_length,\n",
    "            trainable=False,\n",
    "            weights=[embedding_matrix]\n",
    "        )\n",
    "        self.position_embeddings = Embedding(\n",
    "            input_dim=sequence_length,\n",
    "            output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embed_scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_tokens = embedded_tokens * self.embed_scale\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "\n",
    "class TransformerDecoderBlock(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, embedding_matrix, ff_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ff_dim = ff_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
    "        )\n",
    "        self.ffn_layer_1 = layers.Dense(ff_dim, activation=\"relu\")\n",
    "        self.ffn_layer_2 = layers.Dense(embed_dim)\n",
    "\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "\n",
    "        self.embedding = PositionalEmbedding(\n",
    "            embed_dim=embed_dim,\n",
    "            sequence_length=sequence_length,\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_matrix=embedding_matrix\n",
    "        )\n",
    "        self.out = layers.Dense(vocab_size, activation=\"softmax\")\n",
    "\n",
    "        self.dropout_1 = layers.Dropout(0.3)\n",
    "        self.dropout_2 = layers.Dropout(0.5)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, training, mask=None):\n",
    "        inputs = self.embedding(inputs)\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n",
    "            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n",
    "            combined_mask = tf.minimum(combined_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=combined_mask,\n",
    "            training=training,\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "            training=training,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        ffn_out = self.ffn_layer_1(out_2)\n",
    "        ffn_out = self.dropout_1(ffn_out, training=training)\n",
    "        ffn_out = self.ffn_layer_2(ffn_out)\n",
    "\n",
    "        ffn_out = self.layernorm_3(ffn_out + out_2, training=training)\n",
    "        ffn_out = self.dropout_2(ffn_out, training=training)\n",
    "        preds = self.out(ffn_out)\n",
    "        return preds\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [\n",
    "                tf.expand_dims(batch_size, -1),\n",
    "                tf.constant([1, 1], dtype=tf.int32),\n",
    "            ],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "class ImageCaptioningModel(Model):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.encoder = TransformerEncoderBlock(embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=2)\n",
    "        self.decoder = TransformerDecoderBlock(sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE, embed_dim=EMBED_DIM, embedding_matrix=embedding_matrix, ff_dim=FF_DIM, num_heads=8)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        image_features, input_captions = inputs\n",
    "        image_features = layers.Reshape((-1, 1))(image_features)\n",
    "        encoder_out = self.encoder(image_features, training=True)\n",
    "        mask = tf.math.not_equal(input_captions, 0)\n",
    "        output = self.decoder(input_captions, encoder_out, training=True, mask=mask) \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImageCaptioningModel()\n",
    "\n",
    "model.compile(loss = SparseCategoricalCrossentropy(), optimizer= Adam(0.001), metrics=['accuracy'] )\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    # TensorBoard(\n",
    "    #     log_dir='./tensorboard_logs/cnn_transformer_training/',\n",
    "    #     histogram_freq=1,\n",
    "    #     write_graph=True,\n",
    "    #     update_freq='epoch'\n",
    "    # ),\n",
    "]\n",
    "\n",
    "training = model.fit(\n",
    "    train_data,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=STEPS,\n",
    "    validation_data=val_data,\n",
    "    validation_steps=VAL_STEPS,\n",
    "    callbacks= callbacks,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "# Plotting Loss\n",
    "axes[0].plot(training.history['loss'], marker='o', color='b', label='Training Loss')\n",
    "# axes[0].plot(training.history['val_loss'], marker='o', color='r', label='Validation Loss')\n",
    "axes[0].set_title('Model Loss')\n",
    "axes[0].set_xlabel('Epochs')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plotting Accuracy\n",
    "axes[1].plot(training.history['accuracy'], marker='s', color='g', label='Training Accuracy')\n",
    "# axes[1].plot(training.history['val_accuracy'], marker='s', color='orange', label='Validation Accuracy')\n",
    "axes[1].set_title('Model Accuracy')\n",
    "axes[1].set_xlabel('Epochs')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = choice(val_images)\n",
    "test_features = val_feature_map[test_image]\n",
    "target_captions = val_caption_map[test_image]\n",
    "\n",
    "display_image('datasets/coco/val2014/', test_image)\n",
    "\n",
    "greedy, beam, true_captions = generate_and_evaluate_caption(model, test_features, target_captions, idx_to_word, SEQ_LENGTH)\n",
    "print(greedy[0], '\\n')\n",
    "print(beam[0], '\\n')\n",
    "\n",
    "for caption in true_captions: \n",
    "    print(caption)\n",
    "    \n",
    "print(greedy[2], beam[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_captions, true_captions = generate_captions(model, val_images[:1024], val_feature_map, val_caption_map, idx_to_word, batch_size=32)\n",
    "validation_score = evaluate_captions(predicted_captions, true_captions)\n",
    "print(validation_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ([0.6566525747696405,\n",
    "  0.46475125022830116,\n",
    "  0.311301620339562,\n",
    "  0.21286200107485656],\n",
    " 0.41116280406869266) first iteration 5 epochs \n",
    " - ([0.6641633469587254,\n",
    "  0.4708016193944267,\n",
    "  0.31741534441381347,\n",
    "  0.21806740313687273],\n",
    " 0.4172896946730811) 13 epochs\n",
    " - ([0.6670269463236829,\n",
    "  0.47334294126193033,\n",
    "  0.32004501765654514,\n",
    "  0.2204260870604908],\n",
    " 0.41942539024956743) 10 epochs with 16 attention heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('saved_models/transformer_decoder_v2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
